{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2' #Trying to reduce tensorflow warnings\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "# import hw_utils # LOADS HW CODE (helps de-clutter this notebook)\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from pathlib import Path\n",
    "\n",
    "# useful structures and functions for experiments \n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "\n",
    "# specific machine learning functionality\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.utils.layer_utils import count_params\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "try:\n",
    "  from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "  from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "except:\n",
    "  print(\"transformers library not installed, installing through pip\")\n",
    "  !pip install transformers\n",
    "  from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "  from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../data/20220213_074247/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained('../data/20220213_074247/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('../data/20220213_074247/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tf.Tensor([[50256]], shape=(1, 1), dtype=int32)\n",
      "Generated text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' in a nutshell, is this music, as I said before, a phenomenon. IfI can single out only the first two parts, I have the idea. IfI can trace them to Don Giovanni and indirectly trace him, then it would be conceivable that he was a deceiver. But I do believe that this is precisely what I need in order to prompt him to'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# Input text\n",
    "input_text = tokenizer.bos_token\n",
    "\n",
    "# Tokenize Input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "print(\"input_ids\",input_ids)\n",
    "\n",
    "# Generate outout\n",
    "outputs = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True,\n",
    "    max_length=75, \n",
    "    top_p=0.80, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "display(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative_env",
   "language": "python",
   "name": "generative_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
